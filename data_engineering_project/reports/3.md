---
title: "Report 3 - Wrangling"
author: "Quinten Cabo"
date: \today
urlcolor: #4077C0
---

# Making a torch dataloader

I will mainly improve the quality of the data by improving how easy the data will be to load.
To do this I want to create a [torch dataloader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). 
When I was taught machine learning we always had to use data loaders. Sometimes you were unlucky, and you did not have a dataloader for your dataset. 
I remember the processes of creating your own dataset to always be very annoying. 
Thus, I think I would make the data scientist very happy by providing a dataloader as the main way to load the data from knmi.

## Dataloader wishlist/description

In the constructor of this dataloader the data scientist should be able to say which keys they want, from which stations and from which date range (from, until).
The dataloader will then go download and cache the datasets from that date range and the keys you downloaded. 

The resulting shape of the data for the data scientist will be $\(n,s,k,69\)$ where 

- $n$ is based on how many readings there are in the specified date range
- $s$ is the number of valid stations asked for
- $k$ is the number of different keys specified 

The label of each row in the pytorch dataset will be the time the value was recorded. The labels dataset will have n rows.

Effectivity the dataloader will provide the data scientist with the ability to only load the part of the dataset they wish for. 
This means that we might only send a subset of the entire dataset from the knmi to the data scientist.


## Resulting data model

Because I want the data scientist to query the dataset using the data loader I think a relational data model is the best fit.
Using a relation database I can easily query the central database for exactly the data that the data scientist requested.
A relational database additionally provides an easy way to combine the data from multiple nc files because it can just be added to the table.
The database schema currently only has a single table. The schema of this table can be found in the appendix.
I do not need more than one table. Each row is the measurements of a station at a certain point in time. 
You can already filter the columns you want in the select statement. 
Having more tables will only needlessly increase complexity. 

To create this relational database to facilitate the experience with the dataloader I will create a pipeline.
This pipeline will add the new data from the knmi stream (in the form of nc files) to a central relational database hosted somewhere.

When the data scientist loads the data using the torch dataloader the server shall answer the query by querying this central database.
Then once the data has been collected on the server it will be sent back to the data scientist in the form of a parquet file.

The rest of this document will only discuss creating the pipeline and not the actual torch dataloader.
It makes more sense to discuss the actual data loader in assignment 4 which is about actually serving the data.

# Pipeline 

Remember, these nc files are basically measurements from s stations of (currently) 94 variables at a single point in time.
To be able to query the data later, I want to save the measurements of each station at a point in time as a separate row in the database.
This means I have to transform the nc file into a list of table rows that can be inserted into the database. 
This is the job of the pipeline.

The pipeline is called upon when an item actually comes in from the knmi stream. The pipeline consists of the following steps: 

1. Download and save the nc file by sending the required requests.
2. Load the nc file
3. Create a list of rows to insert into the database from the nc file
4. Get the time value (there is only 1 per nc file)
5. Create each row by iterating over the column names and for each station: 
   1. Deal with the fact that the vectors are of shape (69,1) instead of just (69,)
   2. Deal with the masked values and add missing values as None to the row to insert
   3. Do not touch string as these vectors are actually (69,) so just put them in the row to insert
6. Insert the list of rows into a relational database

It might look a bit simple now, but I spend a lot of time thinking about this, exploring the data and considering different options to finally get to this simplicity. For instance, I looked at many vector databases or making a larger h5 file before finally settling on a relational model.

## Actual implementation details of the pipeline 

To create this pipeline I created two functions. The first function `process_nc_file` with the type $Path \rightarrow List\[ List\[str \| int \| float\]\]$
This function takes a file path and executes the first 5 steps. Then the second function, `insert_nc_filerows` with type $List\[ List\[str \| int \| float\]\] \rightarrow \(\)$ inserts the result from `process_nc_file` into the relational database. In doing so it completes step 6 of the pipeline.

For this code it is less important that it is easy to break because it will probably not be rewritten that much. 
This is because I wrote the functions in such a way that they can deal with a dynamic shape of the nc file. 
The code can for instance handle if the number of stations changes.

When knmi decides to add more data to their nc files the wrangler won't break. But you do have to add the new nc database name to the `nc_keys_to_save` tuple in wrangle.py and to the schema in the database to actually save this new data.

the two functions can be found in the appendix or in the [git repository](https://github.com/tintin10q/data-engineering-NWI-IMC073)

### Configuration files

Making things data driving is good because it makes the code much more resilient to change.
With data driven I mean read from a config file.
During the wrangling step of the project I wanted to make `nc_keys_to_save` data driven. 
Then I thought that if I was going to make the list of keys data driven I might as well make the configuration for the stream knmi connection data driven as well. 

To this end I created two config files `auth.toml` and `config.toml`. The idea is that `auth.toml` holds secret data which should not be commited to source control while `config.toml` only holds public knowledge data which can be commited to source control. A table with the keys of the configuration can be found in the appendix.
However, in the end I decided against putting `nc_keys_to_save` into a configuration file.
I decided against this because I dynamically generate the sql insert statement from the key names. 
Do not worry I still use prepared statements, but I construct the number of `?` holes and column names dynamically from the 
`nc_keys_to_save` list. For this reason it is better for security if it is a bit more difficult to edit the column names.

## Performance of the pipeline

A single nc file is about $419145$ bytes on average. It takes my pipeline on average 1.43 seconds to process and nc file and then on average 2.54 seconds to save the rows into the database. 
These are just simple benchmarks using python `time.time` and this is just running on my laptop. 

Clearly, the processing of the nc file could be sped up significantly as this is a naive python script. But I do not think that this is necessary as a nc file comes in only every 10 minutes.

## Tools used in the pipeline

I wrote the two wrangle functions in python as python is a langauge that is easy to write with many dependencies. 

For the database I decided to use [duckdb](https://duckdb.org). I wanted to try out this technology after the lectures. After trying it I can say that I had a good experience with it. I love the documentation it is really well written. 

I also used the [netCDF4](https://pypi.org/project/netCDF4/) library to actually load the nc files. I use [paho-mqtt](https://pypi.org/project/paho-mqtt/) to listen to the data stream from knmi. 

## Difficulties

Everything went smoothly except for one thing. 
There are two fields within the nc files that have an `-` in them. Duckdb does not support this directly. 
I had two options. Either change to _ or you quote the column names like `"W10-10"` and `"ww-10"`. 
I decided for quoting as changing the name seems like a bad idea because no one using the dataset will expect 
that the names have been changed. I suspect that changing the names of columns will cause confusion. Quoting is a bit inconvenient for me, but it should be done to not create confusion for the end users.

I did the quoting manually for the table creation, but I dynamically generate the sql query based on the keys specified to save in `key_name_order`. 
So to fix it here I used the following regular expression substitution: `/([A-Z0-9a-z]+-[A-Z0-9a-z]+)/"\1"/`

# Airflow 

During lecture 4 directed acyclic graphs (dag) were discussed. Two tools were mentioned: Airflow and DBT.
I found this concept very interesting and I decided to investigate Airflow because that tool appealed to me more.

However, using Airflow did not go well for me. 
After trying to install Airflow for about 2 hours using pip it seemed like it installed correctly, but then I still got weird import errors. 
So I gave up on the pip install and tried with docker. 
Airflow is the only python package I have ever seen that needs a [constraints file](https://raw.githubusercontent.com/apache/airflow/constraints-main/constraints-3.10.txt) to install.
Installing with docker went a lot better but this installation came with many examples.
These were honestly very overwhelming. 
I did make an Airflow python file that executed part of my pipeline but using Airflow to solve my problem just felt overly complicated.
Using Airflow is not so difficult, but it is bad at hiding that there is a large amount of complexity within.
I felt like the pipeline I wanted to make did not need all the complexity that Airflow brings. 
Besides 



Simple, complexity not needed, but I can see how it is useful. 

I did not see a way to trigger from a stream.



Just a python package. The dependencies can be easy installed using [poetry](https://python-poetry.org/) and if you would really want to this process could be put into a container.


I actually did:

Installing airflow with https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html
They really make a point of not changing the airflow version, so I will just run the pip command they give in the current virtual env I have.

pip install "apache-airflow[celery]==2.9.1" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.9.1/constraints-3.10.txt"

It failed once and then said I needed to install wheels, and so I did, and then it seems to work
It did not, during import of airflow I got `ModuleNotFoundError: No module named 'airflow.models'; 'airflow' is not a package` which is weird because it definity got installed.
Ok so docker then?

I ended up following this

https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html

Ok then I decided that airflow was way to complex for this. I don't think the complexity is required.

# Appendix

## Schema of the Measurement table:

```sql
CREATE TABLE Measurement (
    station    TEXT NOT NULL,
    time BIGINT NOT NULL CHECK ( time > 0 ),
    wsi        TEXT,
    stationname  TEXT,
    lat        DOUBLE,
    lon        DOUBLE,
    height     DOUBLE,
    D1H        DOUBLE,
    dd         DOUBLE,
    dn         DOUBLE,
    dr         DOUBLE,
    dsd        DOUBLE,
    dx         DOUBLE,
    ff         DOUBLE,
    ffs        DOUBLE,
    fsd        DOUBLE,
    fx         DOUBLE,
    fxs        DOUBLE,
    gff        DOUBLE,
    gffs       DOUBLE,
    h          DOUBLE,
    h1         DOUBLE,
    h2         DOUBLE,
    h3         DOUBLE,
    hc         DOUBLE,
    hc1        DOUBLE,
    hc2        DOUBLE,
    hc3        DOUBLE,
    n          DOUBLE,
    n1         DOUBLE,
    n2         DOUBLE,
    n3         DOUBLE,
    nc         DOUBLE,
    nc1        DOUBLE,
    nc2        DOUBLE,
    nc3        DOUBLE,
    p0         DOUBLE,
    pp         DOUBLE,
    pg         DOUBLE,
    pr         DOUBLE,
    ps         DOUBLE,
    pwc        DOUBLE,
    Q1H        DOUBLE,
    Q24H       DOUBLE,
    qg         DOUBLE,
    qgn        DOUBLE,
    qgx        DOUBLE,
    qnh        DOUBLE,
    R12H       DOUBLE,
    R1H        DOUBLE,
    R24H       DOUBLE,
    R6H        DOUBLE,
    rg         DOUBLE,
    rh         DOUBLE,
    rh10       DOUBLE,
    Sav1H      DOUBLE,
    Sax1H      DOUBLE,
    Sax3H      DOUBLE,
    Sax6H      DOUBLE,
    sq         DOUBLE,
    ss         DOUBLE,
    Sx1H       DOUBLE,
    Sx3H       DOUBLE,
    Sx6H       DOUBLE,
    t10        DOUBLE,
    ta         DOUBLE,
    tb         DOUBLE,
    tb1        DOUBLE,
    Tb1n6      DOUBLE,
    Tb1x6      DOUBLE,
    tb2        DOUBLE,
    Tb2n6      DOUBLE,
    Tb2x6      DOUBLE,
    tb3        DOUBLE,
    tb4        DOUBLE,
    tb5        DOUBLE,
    td         DOUBLE,
    td10       DOUBLE,
    tg         DOUBLE,
    tgn        DOUBLE,
    Tgn12      DOUBLE,
    Tgn14      DOUBLE,
    Tgn6       DOUBLE,
    tn         DOUBLE,
    Tn12       DOUBLE,
    Tn14       DOUBLE,
    Tn6        DOUBLE,
    tsd        DOUBLE,
    tx         DOUBLE,
    Tx12       DOUBLE,
    Tx24       DOUBLE,
    Tx6        DOUBLE,
    vv         DOUBLE,
    W10        DOUBLE,
    "W10-10"   DOUBLE,
    ww         DOUBLE,
    "ww-10"    DOUBLE,
    zm         DOUBLE,
    PRIMARY KEY (time, station)
    );
```

## Table of configuration files

| Key                 | File          | Description                                      |
|---------------------|---------------|--------------------------------------------------|
| CLIENT_ID           | `auth.toml`   | Identify yourself to the mqtt stream             |
| TOPIC               | `config.toml` | Topic of the mqtt stream                         | 
| BROKER_DOMAIN       | `config.toml` | Domain of the mqtt stream                        | 
| TOKEN               | `auth.toml`   | Secret token used to connect to the mqtt stream  |
| FILE_DOWNLOAD_TOKEN | `auth.toml`   | Secret token used to get nc file download links  |
