---
title: "Report 3"
author: "Quinten Cabo"
date: \today
---

Empty Page


Now that you have assessed the quality of your dataset, you have to decide how you want to wrangle and reshape it. What to wrangle, and how, as well as the final shape of the dataset all depend on the use-case that you have chosen. Here are some questions to help you get started:

    Imagine you are the end user of this dataset (e.g., a data scientist/analyst), what would be the easiest structure of the dataset to use in the intended downstream task?

    Is your dataset semi-structured (e.g., XML/JSON)? Do you want the resulting dataset to be structured (e.g., Parquet, CSV)? How do you plan to convert it?

    If your dataset comes from multiple sources, do you intend to combine them all into one dataset? How do you want to combine multiple (possibly different) schema’s?

    Did you notice any issues or peculiarities with your dataset in the quality assessment stage, which you can solve by transforming the data somehow? For example, are all the dates in the same format? Are all the temperatures in the same unit (Celsius/Fahrenheit)?

    Can you process all of the data in bulk, or do you have to use a batch-based, streaming or distributed approach? Related to that, how well does your pipeline or workflow scale, if the input data were to grow bigger?

    Which tool or technology fits best with your required wrangling and reshaping steps?

    Can you automate the wrangling and reshaping stage, for instance to handle updates or simplify making small changes to the output data product?

    Would the dataset benefit from any (de-)normalization? For example, if your dataset contains a table with “orders”, containing an order ID and customer ID, and a table with “ordered_items”, which contains an order ID and an item ID, would it help to join “orders” and “ordered_items” so that it is easier to find which customer ordered which items? Or, vice-versa, is your dataset already pre-joined, and can you un-join it by creating two tables?


Some common tools that can be useful in the wrangling & reshaping stage are the following:

    SQL

    Python + Pandas/Polars/?

    Spark

    Airflow

    dbt


Initially, your weekly assignments will be graded on a pass/fail basis. The final report consists of all (polished) assignments. In the final report, the part on quality assessment will be graded based on the following scales:

    Mistakes were made in wrangling and/or transforming the data, introducing errors into the dataset and affecting the usability of the final product. 

    The transformations were performed correctly, but did not take pre-existing issues of the data into account, thus propagating errors into the output data product.

    The transformations are performed correctly. Suitable transformations were identified, e.g., appropriate (fuzzy) join keys were chosen, and the data was reshaped in a sensible manner. The final data is of high quality.

Use case:

    The final data product does not support the intended use case; the end-user will need to perform additional pre-processing to get the data ready for their application. 

    The data product supports the intended use case better than the original data, but still requires some additional processing. 

    The final data product clearly supports the intended use case; the end-user (e.g., data analyst/scientist) can easily build their application on top. 


Tooling/automation:

    The pipeline is hacked together or the chosen tools are not very well-aligned, and the pipeline will definitely have to be modified to support future changes and other updates.

    The pipeline consists of reasonable tools that did the job, but could be automated or implemented more effectively.

    The appropriate tools were used to perform the wrangling & reshaping task. The pipeline can easily be automated to simplify deployment for future changes or other updates.
