---
title: "Report 3 - Wrangling"
author: "Quinten Cabo"
date: \today
urlcolor: #4077C0
---


The past data never changes, it is not private data, new data is added to the duckdb database. 
The dataloader can load this new data. 

# His text:


Your full data pipeline should now be up and running, where you 1) loaded your input data, 2) assessed its quality, 3) reshaped/wrangled the data, and 4) thought about how to serve the data. While the data engineer’s job seems over now, we still have to consider how to handle changes to the dataset in the future. Your dataset might require regular updates, or might be subject to GDPR deletion requests.

In this assignment, you will have to think about the changes your dataset may face, and how you would handle them. Note that you will not have to implement the lifecycle strategy of your choice, it is enough to explain your plans in detail and argue why your proposal fits your dataset and use case.

Besides arguing a suitable lifecycle strategy for your current (likely small) dataset, imagine you obtain a much larger collection of data in the same structure, e.g. multiple gigabytes or terabytes. Also, it will now be updated somewhat regularly. How would you handle your lifecycle in this hypothetical case?

To answer both parts of this assignment, we have again compiled a list of example questions to inspire you and help you get started. However, we reiterate: 1) the questions may not be exhaustive and are only intended to help you get started, and 2) try to formulate your assignment as part of your final report, don’t enumerate a list of answers to the example questions.

The hypotetical scenario is not hypothetical I am already dealing with a dataset that is updated often.


## Example Questions:

- How often does your dataset get updated? Does your processed data product need to be updated just as often, or can it be updated less often (Data Freshness)? How long will it stay relevant? Is there a cost associated with keeping data forever? Can you delete data at some point?
- Assume your dataset will be updated. What kind of updates could there be? Think about adding, updating, and deleting data, but also about schema changes. Is the format of your data appropriate (Row/Columnar Layout)? How will you update your processed data product? Do you always have to recompute the entire pipeline, do you only have to run the pipeline for the updates (Lambda Architecture / Microbatching), or do you need real-time updates (Streaming)? Why?
- How does the size of your data affect your strategy? What if your data was bigger/smaller?
- How will you handle errors in your data pipeline when processing updates? For instance, your quality assessment stage detects new issues that were not encountered before, or your wrangling/reshaping code crashes?
- What tools would you use to implement your chosen lifecycle strategy

## Grading

Lifecycle assessment:

1. The student has assessed the dataset lifecycle poorly, e.g. assumptions were made that the dataset would remain static even if this is not necessarily true.
2. The student has adequately assessed the lifecycle implications of their current datasets, but has not given any thought to hypothetical scenarios such as larger input datasets or more frequent updates.
3. The student has assessed the lifecycle of the dataset and corresponding processed data product in-depth. If applicable, some of the more realistic hypothetical situations were assessed, for example, if the dataset size grows very large.

Strategy:

1. The student’s chosen strategy does not align with the outcomes of the lifecycle assessment. For instance, the student proposes to run a lambda architecture but their data pipeline contains joins/aggregations that require the pipeline to be fully recomputed.
2. The student’s chosen strategy works decently for the observed lifecycle requirements, but it is not cost-effective or not substantiated well enough. For instance, the strategy is to re-run the full pipeline every time, while delta-updates could have done the trick.
3. The student has chosen a suitable strategy that aligns perfectly with the outcomes of their assessment of the lifecycle, and they have exhaustively justified their choice.
 